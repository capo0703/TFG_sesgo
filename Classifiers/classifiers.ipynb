{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e79c1c",
   "metadata": {},
   "source": [
    "# Clasificadores de sesgo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7018c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Configuración de entorno\n",
    "# ============================\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Evita warnings de paralelismo en tokenizers de HuggingFace\n",
    "\n",
    "# ============================\n",
    "# Librerías Numéricas y DataFrames\n",
    "# ============================\n",
    "import numpy as np  # Computación numérica\n",
    "import pandas as pd  # Manipulación de datos tabulares\n",
    "\n",
    "# ============================\n",
    "# Deep Learning y manejo de modelos\n",
    "# ============================\n",
    "import torch  # PyTorch: Deep learning y tensor computations\n",
    "import torch.nn as nn  # Módulos de redes neuronales en PyTorch\n",
    "from torch.utils.data import Dataset, DataLoader  # Utilidades para datasets y dataloaders en PyTorch\n",
    "from torch.optim import AdamW  # Optimizador AdamW de PyTorch\n",
    "from torch.amp import autocast, GradScaler  # Mixed precision training\n",
    "\n",
    "# ============================\n",
    "# Modelos preentrenados y NLP\n",
    "# ============================\n",
    "from transformers import AutoTokenizer, AutoModel  # Tokenización y modelos preentrenados de transformers (HuggingFace)\n",
    "\n",
    "# ============================\n",
    "# Ciencia de Datos y Machine Learning\n",
    "# ============================\n",
    "from sklearn.preprocessing import StandardScaler  # Normalización de datos\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,  # K-Fold estratificado\n",
    "    KFold,            # K-Fold simple\n",
    "    ParameterGrid     # Grid de parámetros para búsqueda de hiperparámetros\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score  # Métricas de evaluación\n",
    "from sklearn.utils.class_weight import compute_class_weight  # Cálculo de pesos de clase\n",
    "\n",
    "# ============================\n",
    "# Muestreo y balanceo de clases\n",
    "# ============================\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN  # Técnicas combinadas de sobremuestreo y submuestreo\n",
    "from imblearn.over_sampling import RandomOverSampler  # Sobremuestreo aleatorio\n",
    "from imblearn.under_sampling import RandomUnderSampler  # Submuestreo aleatorio\n",
    "from imblearn.pipeline import Pipeline  # Pipelines de balanceo\n",
    "\n",
    "# ============================\n",
    "# Utilidades varias\n",
    "# ============================\n",
    "import json  # Manejo de archivos y datos JSON\n",
    "from tqdm import tqdm  # Barra de progreso para loops\n",
    "from collections import defaultdict  # Diccionario con valores por defecto\n",
    "import os.path  # Operaciones de sistema de archivos\n",
    "import warnings  # Manejo de warnings\n",
    "\n",
    "# ============================\n",
    "# (Opcional) Otros modelos ML\n",
    "# ============================\n",
    "import lightgbm as lgb  # Gradient boosting, solo si se usa LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d040a34",
   "metadata": {},
   "source": [
    "## Funciones generales y definiciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b40a5f",
   "metadata": {},
   "source": [
    "### Varables globales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== CONFIGURACIÓN ===================\n",
    "\n",
    "# This variables need to be adjusted depending on the model and task\n",
    "\n",
    "MODEL_NAME = \"PlanTL-GOB-ES/roberta-base-bne\"\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 2 \n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 256\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "USE_FP16 = True\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc1775",
   "metadata": {},
   "source": [
    "### Clase y funciones de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "\n",
    "class TextTabularDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para tareas que combinan texto (tokenizado) y variables tabulares.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, attention_mask, tabular_data, labels=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.tabular_data = tabular_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'tabular': torch.tensor(self.tabular_data[idx], dtype=torch.float32)\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            return inputs, torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class RobertaWithTabularDeepHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo que combina las salidas de un modelo Roberta/BERT y variables tabulares adicionales, \n",
    "    seguido por una cabeza densa profunda.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, roberta_model, tabular_dim=1, num_classes=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.roberta = AutoModel.from_pretrained(roberta_model)\n",
    "        concat_dim = self.roberta.config.hidden_size + tabular_dim\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(concat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, tabular):\n",
    "        outputs = self.roberta(input_ids=input_ids,\n",
    "                               attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        concat = torch.cat([pooled_output, tabular], dim=1)\n",
    "        return self.head(concat)\n",
    "\n",
    "# Training function\n",
    "\n",
    "def train_and_validate(\n",
    "    model, train_loader, val_loader, class_weights,\n",
    "    epochs=5, lr=2e-5, patience=2\n",
    "):\n",
    "    model.to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    if USE_FP16:\n",
    "        try:\n",
    "            scaler = GradScaler(device_type=\"cuda\")\n",
    "        except TypeError:\n",
    "            scaler = GradScaler()\n",
    "    best_val_f1_macro = -1\n",
    "    best_val_acc = 0\n",
    "    best_val_f1_per_class = None\n",
    "    best_state_dict = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            inputs, labels = batch\n",
    "            labels = labels.to(DEVICE)\n",
    "            with autocast(device_type=\"cuda\", enabled=USE_FP16):\n",
    "                outputs = model(\n",
    "                    input_ids=inputs['input_ids'].to(DEVICE),\n",
    "                    attention_mask=inputs['attention_mask'].to(DEVICE),\n",
    "                    tabular=inputs['tabular'].to(DEVICE)\n",
    "                )\n",
    "                loss = criterion(outputs, labels) / GRADIENT_ACCUMULATION_STEPS\n",
    "            if USE_FP16:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or (step + 1) == len(train_loader):\n",
    "                if USE_FP16:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds, val_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch\n",
    "                labels = labels.to(DEVICE)\n",
    "                with autocast(device_type=\"cuda\", enabled=USE_FP16):\n",
    "                    outputs = model(\n",
    "                        input_ids=inputs['input_ids'].to(DEVICE),\n",
    "                        attention_mask=inputs['attention_mask'].to(DEVICE),\n",
    "                        tabular=inputs['tabular'].to(DEVICE)\n",
    "                    )\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    val_preds.append(preds.cpu().numpy())\n",
    "                    val_true.append(labels.cpu().numpy())\n",
    "        val_preds = np.concatenate(val_preds)\n",
    "        val_true = np.concatenate(val_true)\n",
    "        val_acc = accuracy_score(val_true, val_preds)\n",
    "        val_f1_macro = f1_score(val_true, val_preds, average='macro')\n",
    "        val_f1_per_class = f1_score(val_true, val_preds, average=None)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_f1_macro > best_val_f1_macro:\n",
    "            best_val_f1_macro = val_f1_macro\n",
    "            best_val_acc = val_acc\n",
    "            best_val_f1_per_class = val_f1_per_class\n",
    "            # Save to CPU to save GPU mem\n",
    "            best_state_dict = {k: v.cpu()\n",
    "                               for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\n",
    "                    f\"Early stopping at epoch {epoch+1} (no val_f1_macro improvement in {patience} epochs)\")\n",
    "                break\n",
    "\n",
    "    # Restore best weights before returning\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "    return best_val_acc, best_val_f1_macro, best_val_f1_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8011d3b",
   "metadata": {},
   "source": [
    "### funciones de preproceasado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def procesar_tipos(ruta_phrases: str, num_labels=5):\n",
    "    \"\"\"\n",
    "    Función generalizada para procesar archivos CSV multi-label con columnas de tipo 'label_1', ..., 'label_N'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(ruta_phrases)\n",
    "    df['bias_type'] = df['bias_type'].astype(str)\n",
    "    labeltags = []\n",
    "    for n in range(1, num_labels + 1):\n",
    "        col_name = f'label_{n}'\n",
    "        labeltags.append(col_name)\n",
    "        df[col_name] = df['bias_type'].apply(\n",
    "            lambda x: str(n) in x.split(',')).astype(int)\n",
    "    cols_to_drop = [c for c in [\n",
    "        'idNew', 'idPhrase', 'bias_type'] if c in df.columns]\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    df = df.fillna(0)\n",
    "    return df, labeltags\n",
    "\n",
    "\n",
    "def obtener_parejas_berta(ruta_news: str, ruta_phrases: str):\n",
    "    df_news = pd.read_csv(ruta_news)\n",
    "    df_phrases = pd.read_csv(ruta_phrases)\n",
    "    df_news = df_news.drop(columns=['idNew', 'fecha', 'newspaper'])\n",
    "    cols_a_borrar = [\n",
    "        'count_X',\n",
    "        'count_X_log',\n",
    "        'ratio_words_X',\n",
    "        'ratio_sentences_X'\n",
    "    ]\n",
    "    df_news = df_news.drop(columns=cols_a_borrar, errors='ignore')\n",
    "    df_pbias = df_phrases.drop(columns=['idNew', 'idPhrase', 'bias_type'])\n",
    "    df_pbias = df_pbias.rename(columns={'bias': 'label'})\n",
    "    first_emb = df_pbias.columns.get_loc('emb_0')\n",
    "    last_emb = df_pbias.columns.get_loc('emb_383')\n",
    "    df_pbias = df_pbias.drop(df_pbias.columns[first_emb:last_emb+1], axis=1)\n",
    "    first_emb = df_news.columns.get_loc('emb_0')\n",
    "    last_emb = df_news.columns.get_loc('emb_383')\n",
    "    df_news = df_news.drop(df_news.columns[first_emb:last_emb+1], axis=1)\n",
    "    pairs = [(df_news, \"news\"), (df_pbias, \"phrases\")]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a2043",
   "metadata": {},
   "source": [
    "## SMOTETomek vs SMOTEEEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47efe80",
   "metadata": {},
   "source": [
    "### tipo de sesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd62cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== CONFIGURACIÓN ===================\n",
    "MODEL_NAME = \"PlanTL-GOB-ES/roberta-base-bne\"\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ======= PARÁMETROS VARIABLES POR EXPERIMENTO ========\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 256\n",
    "NUM_CLASSES = 2  # Cambia según tu tarea/clases\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "USE_FP16 = True\n",
    "\n",
    "grid_search_params = {\n",
    "    'lr':        [2e-5, 5e-5],\n",
    "    'dropout':   [0.1, 0.3],\n",
    "    'epochs':    [5],\n",
    "}\n",
    "param_grid = list(ParameterGrid(grid_search_params))\n",
    "balanceadores = {\n",
    "    \"smoteen\": SMOTEENN(random_state=SEED),\n",
    "    \"smotetomek\": SMOTETomek(random_state=SEED)\n",
    "}\n",
    "\n",
    "# =================== PREPROCESADO DE LOS DATOS ===================\n",
    "\n",
    "phrases_loc = './csv/processed_phrases_embed.csv'\n",
    "df_ptype, labeltags = procesar_tipos(phrases_loc, num_labels=5)\n",
    "\n",
    "first_emb = df_ptype.columns.get_loc('emb_0')\n",
    "last_emb = df_ptype.columns.get_loc('emb_383')\n",
    "df_noemb = df_ptype.drop(df_ptype.columns[first_emb:last_emb+1], axis=1)\n",
    "\n",
    "X = df_noemb.drop(columns=labeltags)\n",
    "tabular_cols = [col for col in X.columns if col != 'text']\n",
    "text_col = 'text'\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[tabular_cols] = scaler.fit_transform(X_scaled[tabular_cols])\n",
    "X_scaled[text_col] = X[text_col]\n",
    "\n",
    "# =================== MAIN TRAINING LOOP ===================\n",
    "for labeltag in labeltags:\n",
    "    print(f\"\\n=== Grid search Roberta para {labeltag} ===\")\n",
    "    y = df_ptype[labeltag].values.astype(int)\n",
    "    df_actual = X_scaled.copy()\n",
    "    df_actual['label'] = y\n",
    "    texts = df_actual[text_col].astype(str).tolist()\n",
    "    tabular_data = df_actual[tabular_cols].values\n",
    "    labels = y\n",
    "    scaler = StandardScaler()\n",
    "    tabular_scaled = scaler.fit_transform(tabular_data)\n",
    "    dummy_tabular = np.zeros((len(df_actual), 1), dtype=np.float32)\n",
    "\n",
    "    for use_tabular in [True, False]:\n",
    "        feature_type = \"all\" if use_tabular else \"embedding\"\n",
    "        print(\n",
    "            f\"\\n>> Entrenando con: {'todos los atributos' if use_tabular else 'solo embeddings de RoBERTa'}\")\n",
    "        X_tab = tabular_scaled if use_tabular else dummy_tabular\n",
    "        for balance_name, sampler in balanceadores.items():\n",
    "            print(f\"\\n>> Balanceador: {balance_name}\")\n",
    "            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "            param_results = defaultdict(list)\n",
    "            save_name = f\"resultados_labeltags_{labeltag}_{balance_name}_{feature_type}.json\"\n",
    "            if os.path.exists(save_name):\n",
    "                try:\n",
    "                    with open(save_name, \"r\", encoding=\"utf-8\") as f:\n",
    "                        prev_res = json.load(f)\n",
    "                    already_done = set(prev_res[\"results\"].keys())\n",
    "                    print(\n",
    "                        f\"[INFO] {save_name} encontrado, {len(already_done)}/{len(param_grid)} combinaciones ya guardadas.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Error leyendo {save_name}: {e}\")\n",
    "                    already_done = set()\n",
    "                    prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "            else:\n",
    "                already_done = set()\n",
    "                prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "            for params in tqdm(param_grid, desc=f\"Grid {balance_name} {feature_type} {labeltag}\"):\n",
    "                params_key = str(params)\n",
    "                if params_key in already_done:\n",
    "                    continue  # Saltar combinaciones ya terminadas\n",
    "                fold_accs, fold_f1_macros, fold_f1s_per_class = [], [], []\n",
    "                for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_tab, labels)):\n",
    "                    train_texts = [texts[idx] for idx in train_idx]\n",
    "                    val_texts = [texts[idx] for idx in val_idx]\n",
    "                    train_tokenized = tokenizer(\n",
    "                        train_texts,\n",
    "                        add_special_tokens=True,\n",
    "                        truncation=True,\n",
    "                        max_length=MAX_LEN,\n",
    "                        padding='max_length',\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    val_tokenized = tokenizer(\n",
    "                        val_texts,\n",
    "                        add_special_tokens=True,\n",
    "                        truncation=True,\n",
    "                        max_length=MAX_LEN,\n",
    "                        padding='max_length',\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    train_input_ids, val_input_ids = train_tokenized[\n",
    "                        'input_ids'], val_tokenized['input_ids']\n",
    "                    train_attention_mask, val_attention_mask = train_tokenized[\n",
    "                        'attention_mask'], val_tokenized['attention_mask']\n",
    "                    train_tabular, val_tabular = X_tab[train_idx], X_tab[val_idx]\n",
    "                    train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
    "\n",
    "                    # ----------- BLOQUE ROBUSTO DE BALANCEO Y MAPEADO DE ÍNDICES ---------------\n",
    "                    unique_classes, counts_classes = np.unique(\n",
    "                        train_labels, return_counts=True)\n",
    "                    print(\n",
    "                        f\"[INFO] Fold {fold_idx} clases: {dict(zip(unique_classes, counts_classes))}\")\n",
    "                    use_balanced = True\n",
    "                    if len(unique_classes) < 2:\n",
    "                        print(\n",
    "                            f\"[WARN] Fold {fold_idx} solo tiene una clase, no se puede balancear. Usando datos originales.\")\n",
    "                        tabular_sampl, labels_sampl = train_tabular, train_labels\n",
    "                        idx_sampl = np.arange(len(train_tabular))\n",
    "                        use_balanced = False\n",
    "                    else:\n",
    "                        min_samples = np.min(counts_classes)\n",
    "                        k_neighbors = min(\n",
    "                            5, min_samples-1) if min_samples > 1 else 1\n",
    "                        if hasattr(sampler, \"smote\"):\n",
    "                            sampler.set_params(\n",
    "                                **{\"smote__k_neighbors\": k_neighbors})\n",
    "                        try:\n",
    "                            tabular_sampl, labels_sampl = sampler.fit_resample(\n",
    "                                train_tabular, train_labels)\n",
    "                            if tabular_sampl.shape[0] == 0:\n",
    "                                print(\n",
    "                                    f\"[WARN] Balanceador dejó fold {fold_idx} vacío. Usando datos originales.\")\n",
    "                                tabular_sampl, labels_sampl = train_tabular, train_labels\n",
    "                                idx_sampl = np.arange(len(train_tabular))\n",
    "                                use_balanced = False\n",
    "                            else:\n",
    "                                from sklearn.neighbors import NearestNeighbors\n",
    "                                nn_model = NearestNeighbors(n_neighbors=1)\n",
    "                                nn_model.fit(train_tabular)\n",
    "                                _, idx_sampl = nn_model.kneighbors(\n",
    "                                    tabular_sampl)\n",
    "                                idx_sampl = idx_sampl.flatten()\n",
    "                        except Exception as e:\n",
    "                            print(\n",
    "                                f\"[WARN] Error balanceando fold {fold_idx}: {e}. Usando datos originales.\")\n",
    "                            tabular_sampl, labels_sampl = train_tabular, train_labels\n",
    "                            idx_sampl = np.arange(len(train_tabular))\n",
    "                            use_balanced = False\n",
    "                    # ---------------------------------------------------------------------------\n",
    "\n",
    "                    train_input_ids_sampl = train_input_ids[idx_sampl]\n",
    "                    train_attention_mask_sampl = train_attention_mask[idx_sampl]\n",
    "\n",
    "                    # Construir datasets\n",
    "                    train_dataset = TextTabularDataset(\n",
    "                        train_input_ids_sampl, train_attention_mask_sampl, tabular_sampl, labels_sampl\n",
    "                    )\n",
    "                    val_dataset = TextTabularDataset(\n",
    "                        val_input_ids, val_attention_mask, val_tabular, val_labels\n",
    "                    )\n",
    "                    train_loader = DataLoader(\n",
    "                        train_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        num_workers=4,\n",
    "                        pin_memory=True\n",
    "                    )\n",
    "                    val_loader = DataLoader(\n",
    "                        val_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        num_workers=4,\n",
    "                        pin_memory=True\n",
    "                    )\n",
    "                    present_classes = np.unique(labels_sampl)\n",
    "                    weights_present = compute_class_weight(\n",
    "                        class_weight='balanced',\n",
    "                        classes=present_classes,\n",
    "                        y=labels_sampl\n",
    "                    )\n",
    "                    full_weights = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "                    for cls, w in zip(present_classes, weights_present):\n",
    "                        full_weights[int(cls)] = w\n",
    "                    fold_class_weights = torch.tensor(\n",
    "                        full_weights, dtype=torch.float32, device=DEVICE)\n",
    "                    model = RobertaWithTabularDeepHead(\n",
    "                        tabular_dim=train_tabular.shape[1],\n",
    "                        num_classes=NUM_CLASSES,\n",
    "                        dropout=params['dropout']\n",
    "                    )\n",
    "                    val_acc, val_f1_macro, val_f1_per_class = train_and_validate(\n",
    "                        model,\n",
    "                        train_loader,\n",
    "                        val_loader,\n",
    "                        class_weights=fold_class_weights,\n",
    "                        epochs=params['epochs'],\n",
    "                        lr=params['lr']\n",
    "                    )\n",
    "                    fold_accs.append(val_acc)\n",
    "                    fold_f1_macros.append(val_f1_macro)\n",
    "                    fold_f1s_per_class.append(val_f1_per_class)\n",
    "                if len(fold_accs) == 0:\n",
    "                    print(\n",
    "                        f\"[WARN] Ningún fold válido para {params} en {labeltag} con {balance_name} y {'all' if use_tabular else 'embedding'}\")\n",
    "                    continue\n",
    "                mean_acc = float(np.mean(fold_accs))\n",
    "                mean_f1_macro = float(np.mean(fold_f1_macros))\n",
    "                std_acc = float(np.std(fold_accs))\n",
    "                mean_f1_per_class = np.mean(fold_f1s_per_class, axis=0)\n",
    "                mean_f1_per_class_dict = {f'clase_{cl}': float(f1) for cl, f1 in zip(\n",
    "                    sorted(np.unique(labels)), mean_f1_per_class)}\n",
    "                result = {\n",
    "                    'params': params,\n",
    "                    'mean_acc': mean_acc,\n",
    "                    'mean_f1_macro': mean_f1_macro,\n",
    "                    'std_acc': std_acc,\n",
    "                    'mean_f1_per_class': mean_f1_per_class_dict,\n",
    "                    'fold_results_acc': [float(x) for x in fold_accs],\n",
    "                    'fold_results_f1_macro': [float(x) for x in fold_f1_macros],\n",
    "                    'fold_results_f1_per_class': [f1.tolist() for f1 in fold_f1s_per_class]\n",
    "                }\n",
    "                prev_res[\"results\"][params_key] = result\n",
    "                # Guardado incremental tras cada combinación\n",
    "                with open(save_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(prev_res, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"[OK] Resultados guardados en {save_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0783ed8",
   "metadata": {},
   "source": [
    "### Orientación de sesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c61805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== CONFIGURACIÓN ===================\n",
    "MODEL_NAME = \"PlanTL-GOB-ES/roberta-base-bne\"\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "USE_FP16 = True\n",
    "\n",
    "news_loc = './csv/news.csv'\n",
    "phrases_loc = './csv/processed_phrases_embed.csv'\n",
    "\n",
    "pairs = obtener_parejas_berta(news_loc, phrases_loc)\n",
    "\n",
    "grid_search_params = {\n",
    "    'lr':        [2e-5, 5e-5],\n",
    "    'dropout':   [0.1, 0.3],\n",
    "    'epochs':    [5],\n",
    "}\n",
    "param_grid = list(ParameterGrid(grid_search_params))\n",
    "balanceadores = {\n",
    "    \"smoteen\": SMOTEENN(random_state=SEED),\n",
    "    \"smotetomek\": SMOTETomek(random_state=SEED)\n",
    "}\n",
    "\n",
    "BATCH_SIZE_CONFIG = {\n",
    "    \"news\": 32,\n",
    "    \"phrases\": 256\n",
    "}\n",
    "MAXLEN_CONFIG = {\n",
    "    \"news\": 512,\n",
    "    \"phrases\": 128\n",
    "}\n",
    "\n",
    "# =================== MAIN TRAINING LOOP ===================\n",
    "# SOLO _all.json: solo atributos tabulares+embeddings (NO solo embeddings)\n",
    "for i, (df, dataset_name) in enumerate(pairs):\n",
    "    print(f\"\\n=========== Processing: {dataset_name} ===========\")\n",
    "    MAX_LEN = MAXLEN_CONFIG[dataset_name]\n",
    "    BATCH_SIZE = BATCH_SIZE_CONFIG[dataset_name]\n",
    "    df = df.fillna(0).reset_index(drop=True)\n",
    "\n",
    "    def label_cat(x):\n",
    "        if x > 0.5:\n",
    "            return 1\n",
    "        elif x < -0.5:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "    df['label_cat'] = df['label'].apply(label_cat)\n",
    "    texts = df['text'].astype(str).tolist()\n",
    "    tabular_cols = [col for col in df.columns if col not in [\n",
    "        'label', 'text', 'label_cat']]\n",
    "    tabular_data = df[tabular_cols].values\n",
    "    labels = df['label_cat'].values.astype(int)\n",
    "    scaler = StandardScaler()\n",
    "    tabular_scaled = scaler.fit_transform(tabular_data)\n",
    "\n",
    "    # SOLO el caso \"all\" (con atributos tabulares)\n",
    "    feature_type = \"all\"\n",
    "    print(f\"\\n>> Entrenando con: todos los atributos (tabulares + embeddings de RoBERTa)\")\n",
    "    X_tab = tabular_scaled\n",
    "    for balance_name, sampler in balanceadores.items():\n",
    "        print(f\"\\n>> Balanceador: {balance_name}\")\n",
    "        kfold = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "        save_name = f\"resultados_{dataset_name}_{balance_name}_{feature_type}.json\"\n",
    "        # --- Cargar resultados previos si existen ---\n",
    "        if os.path.exists(save_name):\n",
    "            try:\n",
    "                with open(save_name, \"r\", encoding=\"utf-8\") as f:\n",
    "                    prev_res = json.load(f)\n",
    "                already_done = set(prev_res[\"results\"].keys())\n",
    "                print(\n",
    "                    f\"[INFO] {save_name} encontrado, {len(already_done)}/{len(param_grid)} combinaciones ya guardadas.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Error leyendo {save_name}: {e}\")\n",
    "                already_done = set()\n",
    "                prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "        else:\n",
    "            already_done = set()\n",
    "            prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "        for params in tqdm(param_grid, desc=f\"Grid {balance_name} {feature_type}\"):\n",
    "            params_key = str(params)\n",
    "            if params_key in already_done:\n",
    "                continue  # Saltar combinaciones ya terminadas\n",
    "            fold_accs, fold_f1_macros, fold_f1s_per_class = [], [], []\n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_tab, labels)):\n",
    "                # Tokenización por split para ahorrar RAM\n",
    "                train_texts = [texts[idx] for idx in train_idx]\n",
    "                val_texts = [texts[idx] for idx in val_idx]\n",
    "                train_tokenized = tokenizer(\n",
    "                    train_texts,\n",
    "                    add_special_tokens=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LEN,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                val_tokenized = tokenizer(\n",
    "                    val_texts,\n",
    "                    add_special_tokens=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LEN,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                train_input_ids, val_input_ids = train_tokenized['input_ids'], val_tokenized['input_ids']\n",
    "                train_attention_mask, val_attention_mask = train_tokenized[\n",
    "                    'attention_mask'], val_tokenized['attention_mask']\n",
    "                train_tabular, val_tabular = X_tab[train_idx], X_tab[val_idx]\n",
    "                train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
    "\n",
    "                # === BLOQUE ROBUSTO DE BALANCEO ===\n",
    "                if np.all(train_tabular == train_tabular[0]):\n",
    "                    print(\n",
    "                        f\"[WARN] Fold {fold_idx}: solo embeddings (dummy tabular). No se balancea.\")\n",
    "                    tabular_sampl, labels_sampl = train_tabular, train_labels\n",
    "                    idx_sampl = np.arange(len(train_tabular))\n",
    "                elif len(np.unique(train_labels)) < 2:\n",
    "                    print(\n",
    "                        f\"[WARN] Fold {fold_idx}: solo hay una clase, no se puede balancear.\")\n",
    "                    tabular_sampl, labels_sampl = train_tabular, train_labels\n",
    "                    idx_sampl = np.arange(len(train_tabular))\n",
    "                else:\n",
    "                    # Ajusta k_neighbors para SMOTE si hay pocos positivos\n",
    "                    bincounts = np.bincount(train_labels)\n",
    "                    min_samples = np.min(bincounts[bincounts > 0])\n",
    "                    k_neighbors = min(\n",
    "                        5, min_samples-1) if min_samples > 1 else 1\n",
    "                    if hasattr(sampler, \"smote\") and getattr(sampler, \"smote\", None) is not None:\n",
    "                        sampler.set_params(\n",
    "                            **{\"smote__k_neighbors\": k_neighbors})\n",
    "                    try:\n",
    "                        tabular_sampl, labels_sampl = sampler.fit_resample(\n",
    "                            train_tabular, train_labels)\n",
    "                        if tabular_sampl.shape[0] == 0:\n",
    "                            print(\n",
    "                                f\"[WARN] Fold {fold_idx}: el balanceador devolvió 0 muestras. No se balancea.\")\n",
    "                            tabular_sampl, labels_sampl = train_tabular, train_labels\n",
    "                            idx_sampl = np.arange(len(train_tabular))\n",
    "                        else:\n",
    "                            from sklearn.neighbors import NearestNeighbors\n",
    "                            nn_model = NearestNeighbors(n_neighbors=1)\n",
    "                            nn_model.fit(train_tabular)\n",
    "                            _, idx_sampl = nn_model.kneighbors(tabular_sampl)\n",
    "                            idx_sampl = idx_sampl.flatten()\n",
    "                    except Exception as e:\n",
    "                        warnings.warn(\n",
    "                            f\"Fold {fold_idx}: error en balanceador: {e}. No se balancea.\")\n",
    "                        tabular_sampl, labels_sampl = train_tabular, train_labels\n",
    "                        idx_sampl = np.arange(len(train_tabular))\n",
    "\n",
    "                train_input_ids_sampl = train_input_ids[idx_sampl]\n",
    "                train_attention_mask_sampl = train_attention_mask[idx_sampl]\n",
    "                train_dataset = TextTabularDataset(\n",
    "                    train_input_ids_sampl, train_attention_mask_sampl, tabular_sampl, labels_sampl\n",
    "                )\n",
    "                val_dataset = TextTabularDataset(\n",
    "                    val_input_ids, val_attention_mask, val_tabular, val_labels\n",
    "                )\n",
    "                train_loader = DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "                val_loader = DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=False,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "                present_classes = np.unique(labels_sampl)\n",
    "                weights_present = compute_class_weight(\n",
    "                    class_weight='balanced',\n",
    "                    classes=present_classes,\n",
    "                    y=labels_sampl\n",
    "                )\n",
    "                full_weights = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "                for cls, w in zip(present_classes, weights_present):\n",
    "                    full_weights[int(cls)] = w\n",
    "                fold_class_weights = torch.tensor(\n",
    "                    full_weights, dtype=torch.float32, device=DEVICE)\n",
    "                model = RobertaWithTabularDeepHead(\n",
    "                    tabular_dim=train_tabular.shape[1],\n",
    "                    num_classes=NUM_CLASSES,\n",
    "                    dropout=params['dropout']\n",
    "                )\n",
    "                val_acc, val_f1_macro, val_f1_per_class = train_and_validate(\n",
    "                    model,\n",
    "                    train_loader,\n",
    "                    val_loader,\n",
    "                    class_weights=fold_class_weights,\n",
    "                    epochs=params['epochs'],\n",
    "                    lr=params['lr']\n",
    "                )\n",
    "                fold_accs.append(val_acc)\n",
    "                fold_f1_macros.append(val_f1_macro)\n",
    "                fold_f1s_per_class.append(val_f1_per_class)\n",
    "            mean_acc = float(np.mean(fold_accs))\n",
    "            mean_f1_macro = float(np.mean(fold_f1_macros))\n",
    "            std_acc = float(np.std(fold_accs))\n",
    "            mean_f1_per_class = np.mean(fold_f1s_per_class, axis=0)\n",
    "            mean_f1_per_class_dict = {f'clase_{cl}': float(f1) for cl, f1 in zip(\n",
    "                sorted(np.unique(labels)), mean_f1_per_class)}\n",
    "            result = {\n",
    "                'params': params,\n",
    "                'mean_acc': mean_acc,\n",
    "                'mean_f1_macro': mean_f1_macro,\n",
    "                'std_acc': std_acc,\n",
    "                'mean_f1_per_class': mean_f1_per_class_dict,\n",
    "                'fold_results_acc': [float(x) for x in fold_accs],\n",
    "                'fold_results_f1_macro': [float(x) for x in fold_f1_macros],\n",
    "                'fold_results_f1_per_class': [f1.tolist() for f1 in fold_f1s_per_class]\n",
    "            }\n",
    "            prev_res[\"results\"][params_key] = result\n",
    "            # Guardado incremental tras cada combinación\n",
    "            with open(save_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(prev_res, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"[OK] Resultados guardados en {save_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c507011e",
   "metadata": {},
   "source": [
    "## Balanceadores mixtos simples (Embeddings vs Embeddings + Tabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0567d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def balanceo_mixto_multi(tabular, labels, input_ids, att_mask, maj_perc=0.7, min_perc=0.6, random_state=42):\n",
    "    \"\"\"\n",
    "    Oversample y undersample tabular, input_ids, att_mask y labels a la vez.\n",
    "    Devuelve: tabular_res, input_ids_res, att_mask_res, y_res\n",
    "    \"\"\"\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    if len(classes) != 2:\n",
    "        raise ValueError(\"Solo soportado para binario\")\n",
    "    maj = classes[np.argmax(counts)]\n",
    "    min_ = classes[np.argmin(counts)]\n",
    "    n_maj = counts[np.argmax(counts)]\n",
    "    target_maj = int(n_maj * maj_perc)\n",
    "    target_min = int(n_maj * min_perc)\n",
    "    # Concatenar features para que todas sean oversampleadas igual\n",
    "    X_all = np.concatenate(\n",
    "        [tabular, input_ids, att_mask], axis=1\n",
    "    )\n",
    "    ros = RandomOverSampler(\n",
    "        sampling_strategy={maj: n_maj, min_: target_min}, random_state=random_state)\n",
    "    rus = RandomUnderSampler(sampling_strategy={\n",
    "                             maj: target_maj, min_: target_min}, random_state=random_state)\n",
    "    X_res, y_res = ros.fit_resample(X_all, labels)\n",
    "    X_res, y_res = rus.fit_resample(X_res, y_res)\n",
    "    # Separar de vuelta\n",
    "    n_tab = tabular.shape[1]\n",
    "    n_ids = input_ids.shape[1]\n",
    "    n_mask = att_mask.shape[1]\n",
    "    tabular_res = X_res[:, :n_tab]\n",
    "    input_ids_res = X_res[:, n_tab:n_tab+n_ids].astype(int)\n",
    "    att_mask_res = X_res[:, n_tab+n_ids:].astype(int)\n",
    "    return tabular_res, input_ids_res, att_mask_res, y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc1153",
   "metadata": {},
   "source": [
    "### Tipo de sesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== CONFIGURACIÓN ===================\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "MODEL_NAME = \"PlanTL-GOB-ES/roberta-base-bne\"\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 2  # Cambia a tu número de clases\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# =============== CARGA Y PREPROCESADO ===============\n",
    "phrases_loc = './csv/processed_phrases_embed.csv'\n",
    "\n",
    "df_ptype, labeltags = procesar_tipos(phrases_loc)\n",
    "\n",
    "first_emb = df_ptype.columns.get_loc('emb_0')\n",
    "last_emb = df_ptype.columns.get_loc('emb_383')\n",
    "df_noemb = df_ptype.drop(df_ptype.columns[first_emb:last_emb+1], axis=1)\n",
    "\n",
    "X = df_noemb.drop(columns=labeltags)\n",
    "tabular_cols = [col for col in X.columns if col != 'text']\n",
    "text_col = 'text'\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[tabular_cols] = scaler.fit_transform(X_scaled[tabular_cols])\n",
    "X_scaled[text_col] = X[text_col]\n",
    "\n",
    "# PARAMS Y BALANCEO\n",
    "grid_search_params = {\n",
    "    'lr':        [2e-5, 5e-5],\n",
    "    'dropout':   [0.1, 0.3],\n",
    "    'epochs':    [5]\n",
    "}\n",
    "param_grid = list(ParameterGrid(grid_search_params))\n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# MAIN TRAINING LOOP\n",
    "for labeltag in labeltags:\n",
    "    print(f\"\\n=== Grid search Roberta para {labeltag} ===\")\n",
    "    y = df_ptype[labeltag].values.astype(int)\n",
    "    df_actual = X_scaled.copy()\n",
    "    df_actual['label'] = y\n",
    "    texts = df_actual[text_col].astype(str).tolist()\n",
    "    tabular_data = df_actual[tabular_cols].values\n",
    "    labels = y\n",
    "    scaler = StandardScaler()\n",
    "    tabular_scaled = scaler.fit_transform(tabular_data)\n",
    "    dummy_tabular = np.zeros((len(df_actual), 1), dtype=np.float32)\n",
    "\n",
    "    for use_tabular in [True, False]:\n",
    "        feature_type = \"all\" if use_tabular else \"embedding\"\n",
    "        print(\n",
    "            f\"\\n>> Entrenando con: {'todos los atributos' if use_tabular else 'solo embeddings de RoBERTa'}\")\n",
    "        X_tab = tabular_scaled if use_tabular else dummy_tabular\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "        param_results = defaultdict(list)\n",
    "        save_name = f\"comparativa_{labeltag}_{feature_type}_balanceo_mixto.json\"\n",
    "        if os.path.exists(save_name):\n",
    "            try:\n",
    "                with open(save_name, \"r\", encoding=\"utf-8\") as f:\n",
    "                    prev_res = json.load(f)\n",
    "                already_done = set(prev_res[\"results\"].keys())\n",
    "                print(\n",
    "                    f\"[INFO] {save_name} encontrado, {len(already_done)}/{len(param_grid)} combinaciones ya guardadas.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Error leyendo {save_name}: {e}\")\n",
    "                already_done = set()\n",
    "                prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "        else:\n",
    "            already_done = set()\n",
    "            prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "        for params in tqdm(param_grid, desc=f\"Grid {feature_type} {labeltag}\"):\n",
    "            params_key = str(params)\n",
    "            if params_key in already_done:\n",
    "                continue\n",
    "            fold_accs, fold_f1_macros, fold_f1s_per_class = [], [], []\n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_tab, labels)):\n",
    "                train_texts = [texts[idx] for idx in train_idx]\n",
    "                val_texts = [texts[idx] for idx in val_idx]\n",
    "                train_tokenized = tokenizer(\n",
    "                    train_texts,\n",
    "                    add_special_tokens=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LEN,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='np'\n",
    "                )\n",
    "                val_tokenized = tokenizer(\n",
    "                    val_texts,\n",
    "                    add_special_tokens=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LEN,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                train_input_ids = train_tokenized['input_ids']\n",
    "                train_attention_mask = train_tokenized['attention_mask']\n",
    "                val_input_ids = val_tokenized['input_ids']\n",
    "                val_attention_mask = val_tokenized['attention_mask']\n",
    "                train_tabular, val_tabular = X_tab[train_idx], X_tab[val_idx]\n",
    "                train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
    "\n",
    "                # BALANCEO MIXTO en ambos escenarios (MULTIARRAY)\n",
    "                tabular_sampl, input_ids_sampl, att_mask_sampl, labels_sampl = balanceo_mixto_multi(\n",
    "                    train_tabular, train_labels, train_input_ids, train_attention_mask,\n",
    "                    maj_perc=0.7, min_perc=0.6, random_state=SEED\n",
    "                )\n",
    "                train_input_ids_sampl = torch.tensor(\n",
    "                    input_ids_sampl, dtype=torch.long)\n",
    "                train_attention_mask_sampl = torch.tensor(\n",
    "                    att_mask_sampl, dtype=torch.long)\n",
    "                tabular_sampl = tabular_sampl.astype(np.float32)\n",
    "\n",
    "                # Construir datasets\n",
    "                train_dataset = TextTabularDataset(\n",
    "                    train_input_ids_sampl, train_attention_mask_sampl, tabular_sampl, labels_sampl\n",
    "                )\n",
    "                val_dataset = TextTabularDataset(\n",
    "                    val_input_ids, val_attention_mask, val_tabular, val_labels\n",
    "                )\n",
    "                train_loader = DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "                val_loader = DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=False,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "                present_classes = np.unique(labels_sampl)\n",
    "                weights_present = compute_class_weight(\n",
    "                    class_weight='balanced',\n",
    "                    classes=present_classes,\n",
    "                    y=labels_sampl\n",
    "                )\n",
    "                full_weights = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "                for cls, w in zip(present_classes, weights_present):\n",
    "                    full_weights[int(cls)] = w\n",
    "                fold_class_weights = torch.tensor(\n",
    "                    full_weights, dtype=torch.float32, device=DEVICE)\n",
    "                model = RobertaWithTabularDeepHead(\n",
    "                    tabular_dim=train_tabular.shape[1],\n",
    "                    num_classes=NUM_CLASSES,\n",
    "                    dropout=params['dropout']\n",
    "                )\n",
    "                val_acc, val_f1_macro, val_f1_per_class = train_and_validate(\n",
    "                    model,\n",
    "                    train_loader,\n",
    "                    val_loader,\n",
    "                    class_weights=fold_class_weights,\n",
    "                    epochs=params['epochs'],\n",
    "                    lr=params['lr']\n",
    "                )\n",
    "                fold_accs.append(val_acc)\n",
    "                fold_f1_macros.append(val_f1_macro)\n",
    "                fold_f1s_per_class.append(val_f1_per_class)\n",
    "            if len(fold_accs) == 0:\n",
    "                print(\n",
    "                    f\"[WARN] Ningún fold válido para {params} en {labeltag} con {feature_type}\")\n",
    "                continue\n",
    "            mean_acc = float(np.mean(fold_accs))\n",
    "            mean_f1_macro = float(np.mean(fold_f1_macros))\n",
    "            std_acc = float(np.std(fold_accs))\n",
    "            mean_f1_per_class = np.mean(fold_f1s_per_class, axis=0)\n",
    "            mean_f1_per_class_dict = {f'clase_{cl}': float(f1) for cl, f1 in zip(\n",
    "                sorted(np.unique(labels)), mean_f1_per_class)}\n",
    "            result = {\n",
    "                'params': params,\n",
    "                'mean_acc': mean_acc,\n",
    "                'mean_f1_macro': mean_f1_macro,\n",
    "                'std_acc': std_acc,\n",
    "                'mean_f1_per_class': mean_f1_per_class_dict,\n",
    "                'fold_results_acc': [float(x) for x in fold_accs],\n",
    "                'fold_results_f1_macro': [float(x) for x in fold_f1_macros],\n",
    "                'fold_results_f1_per_class': [f1.tolist() for f1 in fold_f1s_per_class]\n",
    "            }\n",
    "            prev_res[\"results\"][params_key] = result\n",
    "            # Guardado incremental tras cada combinación\n",
    "            with open(save_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(prev_res, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"[OK] Resultados guardados en {save_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd6533",
   "metadata": {},
   "source": [
    "### Orientación del sesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fff702",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"PlanTL-GOB-ES/roberta-base-bne\"\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "news_loc = './csv/news.csv'\n",
    "phrases_loc = './csv/processed_phrases_embed.csv'\n",
    "\n",
    "pairs = obtener_parejas_berta(news_loc, phrases_loc)\n",
    "\n",
    "grid_search_params = {\n",
    "    'lr':        [2e-5, 5e-5],\n",
    "    'dropout':   [0.1, 0.3],\n",
    "    'epochs':    [5],\n",
    "}\n",
    "param_grid = list(ParameterGrid(grid_search_params))\n",
    "\n",
    "BATCH_SIZE_CONFIG = {\n",
    "    \"news\": 32,\n",
    "    \"phrases\": 256\n",
    "}\n",
    "MAXLEN_CONFIG = {\n",
    "    \"news\": 512,\n",
    "    \"phrases\": 128\n",
    "}\n",
    "\n",
    "\n",
    "def balanceo_mixto_multi(tabular, labels, input_ids, att_mask, maj_perc=0.7, min_perc=0.6, random_state=42):\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    if len(classes) < 2:\n",
    "        raise ValueError(\"Se requiere al menos dos clases\")\n",
    "    maj = classes[np.argmax(counts)]\n",
    "    min_ = classes[np.argmin(counts)]\n",
    "    n_maj = counts[np.argmax(counts)]\n",
    "    target_maj = int(n_maj * maj_perc)\n",
    "    target_min = int(n_maj * min_perc)\n",
    "    X_all = np.concatenate([tabular, input_ids, att_mask], axis=1)\n",
    "    ros = RandomOverSampler(\n",
    "        sampling_strategy={maj: n_maj, min_: target_min}, random_state=random_state)\n",
    "    rus = RandomUnderSampler(sampling_strategy={\n",
    "                             maj: target_maj, min_: target_min}, random_state=random_state)\n",
    "    X_res, y_res = ros.fit_resample(X_all, labels)\n",
    "    X_res, y_res = rus.fit_resample(X_res, y_res)\n",
    "    n_tab = tabular.shape[1]\n",
    "    n_ids = input_ids.shape[1]\n",
    "    n_mask = att_mask.shape[1]\n",
    "    tabular_res = X_res[:, :n_tab]\n",
    "    input_ids_res = X_res[:, n_tab:n_tab+n_ids].astype(int)\n",
    "    att_mask_res = X_res[:, n_tab+n_ids:].astype(int)\n",
    "    return tabular_res, input_ids_res, att_mask_res, y_res\n",
    "\n",
    "\n",
    "for i, (df, dataset_name) in enumerate(pairs):\n",
    "    print(f\"\\n=========== Processing: {dataset_name} ===========\")\n",
    "    MAX_LEN = MAXLEN_CONFIG[dataset_name]\n",
    "    BATCH_SIZE = BATCH_SIZE_CONFIG[dataset_name]\n",
    "    df = df.fillna(0).reset_index(drop=True)\n",
    "\n",
    "    def label_cat(x):\n",
    "        if x > 0.5:\n",
    "            return 1\n",
    "        elif x < -0.5:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "    df['label_cat'] = df['label'].apply(label_cat)\n",
    "    texts = df['text'].astype(str).tolist()\n",
    "    tabular_cols = [col for col in df.columns if col not in [\n",
    "        'label', 'text', 'label_cat']]\n",
    "    tabular_data = df[tabular_cols].values\n",
    "    labels = df['label_cat'].values.astype(int)\n",
    "    scaler = StandardScaler()\n",
    "    tabular_scaled = scaler.fit_transform(tabular_data)\n",
    "    dummy_tabular = np.zeros((len(df), 1), dtype=np.float32)\n",
    "\n",
    "    for use_tabular in [True, False]:\n",
    "        feature_type = \"all\" if use_tabular else \"embedding\"\n",
    "        print(\n",
    "            f\"\\n>> Entrenando con: {'todos los atributos' if use_tabular else 'solo embeddings de RoBERTa'}\")\n",
    "        X_tab = tabular_scaled if use_tabular else dummy_tabular\n",
    "        kfold = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "        save_name = f\"resultados_{dataset_name}_balanceo_mixto_{feature_type}.json\"\n",
    "        if os.path.exists(save_name):\n",
    "            try:\n",
    "                with open(save_name, \"r\", encoding=\"utf-8\") as f:\n",
    "                    prev_res = json.load(f)\n",
    "                already_done = set(prev_res[\"results\"].keys())\n",
    "                print(\n",
    "                    f\"[INFO] {save_name} encontrado, {len(already_done)}/{len(param_grid)} combinaciones ya guardadas.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Error leyendo {save_name}: {e}\")\n",
    "                already_done = set()\n",
    "                prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "        else:\n",
    "            already_done = set()\n",
    "            prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "        for params in tqdm(param_grid, desc=f\"Grid balanceo_mixto {feature_type}\"):\n",
    "            params_key = str(params)\n",
    "            if params_key in already_done:\n",
    "                continue\n",
    "            fold_accs, fold_f1_macros, fold_f1s_per_class = [], [], []\n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_tab, labels)):\n",
    "                train_texts = [texts[idx] for idx in train_idx]\n",
    "                val_texts = [texts[idx] for idx in val_idx]\n",
    "                train_tokenized = tokenizer(\n",
    "                    train_texts,\n",
    "                    add_special_tokens=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LEN,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='np'\n",
    "                )\n",
    "                val_tokenized = tokenizer(\n",
    "                    val_texts,\n",
    "                    add_special_tokens=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LEN,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                train_input_ids = train_tokenized['input_ids']\n",
    "                train_attention_mask = train_tokenized['attention_mask']\n",
    "                val_input_ids = val_tokenized['input_ids']\n",
    "                val_attention_mask = val_tokenized['attention_mask']\n",
    "                train_tabular, val_tabular = X_tab[train_idx], X_tab[val_idx]\n",
    "                train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
    "\n",
    "                tabular_sampl, input_ids_sampl, att_mask_sampl, labels_sampl = balanceo_mixto_multi(\n",
    "                    train_tabular, train_labels, train_input_ids, train_attention_mask,\n",
    "                    maj_perc=0.7, min_perc=0.6, random_state=SEED\n",
    "                )\n",
    "                train_input_ids_sampl = torch.tensor(\n",
    "                    input_ids_sampl, dtype=torch.long)\n",
    "                train_attention_mask_sampl = torch.tensor(\n",
    "                    att_mask_sampl, dtype=torch.long)\n",
    "                tabular_sampl = tabular_sampl.astype(np.float32)\n",
    "\n",
    "                train_dataset = TextTabularDataset(\n",
    "                    train_input_ids_sampl, train_attention_mask_sampl, tabular_sampl, labels_sampl\n",
    "                )\n",
    "                val_dataset = TextTabularDataset(\n",
    "                    val_input_ids, val_attention_mask, val_tabular, val_labels\n",
    "                )\n",
    "                train_loader = DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "                val_loader = DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=False,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "                present_classes = np.unique(labels_sampl)\n",
    "                weights_present = compute_class_weight(\n",
    "                    class_weight='balanced',\n",
    "                    classes=present_classes,\n",
    "                    y=labels_sampl\n",
    "                )\n",
    "                full_weights = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "                for cls, w in zip(present_classes, weights_present):\n",
    "                    full_weights[int(cls)] = w\n",
    "                fold_class_weights = torch.tensor(\n",
    "                    full_weights, dtype=torch.float32, device=DEVICE)\n",
    "                model = RobertaWithTabularDeepHead(\n",
    "                    tabular_dim=train_tabular.shape[1],\n",
    "                    num_classes=NUM_CLASSES,\n",
    "                    dropout=params['dropout']\n",
    "                )\n",
    "                val_acc, val_f1_macro, val_f1_per_class = train_and_validate(\n",
    "                    model,\n",
    "                    train_loader,\n",
    "                    val_loader,\n",
    "                    class_weights=fold_class_weights,\n",
    "                    epochs=params['epochs'],\n",
    "                    lr=params['lr']\n",
    "                )\n",
    "                fold_accs.append(val_acc)\n",
    "                fold_f1_macros.append(val_f1_macro)\n",
    "                fold_f1s_per_class.append(val_f1_per_class)\n",
    "            mean_acc = float(np.mean(fold_accs))\n",
    "            mean_f1_macro = float(np.mean(fold_f1_macros))\n",
    "            std_acc = float(np.std(fold_accs))\n",
    "            mean_f1_per_class = np.mean(fold_f1s_per_class, axis=0)\n",
    "            mean_f1_per_class_dict = {f'clase_{cl}': float(f1) for cl, f1 in zip(\n",
    "                sorted(np.unique(labels)), mean_f1_per_class)}\n",
    "            result = {\n",
    "                'params': params,\n",
    "                'mean_acc': mean_acc,\n",
    "                'mean_f1_macro': mean_f1_macro,\n",
    "                'std_acc': std_acc,\n",
    "                'mean_f1_per_class': mean_f1_per_class_dict,\n",
    "                'fold_results_acc': [float(x) for x in fold_accs],\n",
    "                'fold_results_f1_macro': [float(x) for x in fold_f1_macros],\n",
    "                'fold_results_f1_per_class': [f1.tolist() for f1 in fold_f1s_per_class]\n",
    "            }\n",
    "            prev_res[\"results\"][params_key] = result\n",
    "            with open(save_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(prev_res, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"[OK] Resultados guardados en {save_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d2d907",
   "metadata": {},
   "source": [
    "## Undersampling (Embeddings vs Embeddings + Tabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ae320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_multi(tabular, labels, input_ids, att_mask, random_state=42):\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    if len(classes) < 2:\n",
    "        raise ValueError(\"Se requiere al menos dos clases\")\n",
    "    min_count = np.min(counts)\n",
    "    sampling_strategy = {cls: min_count for cls in classes}\n",
    "    X_all = np.concatenate([tabular, input_ids, att_mask], axis=1)\n",
    "    rus = RandomUnderSampler(\n",
    "        sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "    X_res, y_res = rus.fit_resample(X_all, labels)\n",
    "    n_tab = tabular.shape[1]\n",
    "    n_ids = input_ids.shape[1]\n",
    "    n_mask = att_mask.shape[1]\n",
    "    tabular_res = X_res[:, :n_tab]\n",
    "    input_ids_res = X_res[:, n_tab:n_tab+n_ids].astype(int)\n",
    "    att_mask_res = X_res[:, n_tab+n_ids:].astype(int)\n",
    "    return tabular_res, input_ids_res, att_mask_res, y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6d931",
   "metadata": {},
   "source": [
    "### Tipo de sesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf45ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"PlanTL-GOB-ES/roberta-base-bne\"\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "phrases_loc = './csv/processed_phrases_embed.csv'\n",
    "\n",
    "\n",
    "def procesar_tipos(ruta_phrases: str):\n",
    "    df_ptype = pd.read_csv(ruta_phrases)\n",
    "    df_ptype['bias_type'] = df_ptype['bias_type'].astype(str)\n",
    "    labeltags = []\n",
    "    for n in range(1, 6):\n",
    "        col_name = f'label_{n}'\n",
    "        labeltags.append(col_name)\n",
    "        df_ptype[col_name] = df_ptype['bias_type'].apply(\n",
    "            lambda x: str(n) in x.split(',')).astype(int)\n",
    "    df_ptype = df_ptype.drop(columns=['idNew', 'idPhrase', 'bias_type'])\n",
    "    df_ptype = df_ptype.fillna(0)\n",
    "    return df_ptype, labeltags\n",
    "\n",
    "\n",
    "df_ptype, labeltags = procesar_tipos(phrases_loc)\n",
    "first_emb = df_ptype.columns.get_loc('emb_0')\n",
    "last_emb = df_ptype.columns.get_loc('emb_383')\n",
    "embedding_cols = [col for col in df_ptype.columns[first_emb:last_emb+1]]\n",
    "\n",
    "grid_search_params = {\n",
    "    'lr':        [2e-5, 5e-5],\n",
    "    'dropout':   [0.1, 0.3],\n",
    "    'epochs':    [5],\n",
    "}\n",
    "param_grid = list(ParameterGrid(grid_search_params))\n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 256\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "USE_FP16 = True\n",
    "\n",
    "for labeltag in labeltags:\n",
    "    print(f\"\\n=== Grid search Roberta para {labeltag} ===\")\n",
    "    y = df_ptype[labeltag].values.astype(int)\n",
    "    texts = df_ptype['text'].astype(str).tolist()\n",
    "    labels = y\n",
    "\n",
    "    X_tab_all = df_ptype.drop(\n",
    "        columns=labeltags + ['text'] + embedding_cols).values\n",
    "    scaler_all = StandardScaler()\n",
    "    X_tab_all_scaled = scaler_all.fit_transform(X_tab_all)\n",
    "    feature_types = [\n",
    "        (\"all\", X_tab_all_scaled),\n",
    "        (\"embedding\", df_ptype[embedding_cols].values)\n",
    "    ]\n",
    "    for feature_type, X_tab in feature_types:\n",
    "        print(\n",
    "            f\"\\n>> Entrenando con: {'todas las features' if feature_type == 'all' else 'solo embeddings'}\")\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "        param_results = defaultdict(list)\n",
    "        save_name = f\"comparativa_{labeltag}_{feature_type}_undersampling.json\"\n",
    "        already_done = set()\n",
    "        if os.path.exists(save_name):\n",
    "            try:\n",
    "                with open(save_name, \"r\", encoding=\"utf-8\") as f:\n",
    "                    prev_res = json.load(f)\n",
    "                already_done = set(prev_res[\"results\"].keys())\n",
    "                print(\n",
    "                    f\"[INFO] {save_name} encontrado, {len(already_done)}/{len(param_grid)} combinaciones ya guardadas.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Error leyendo {save_name}: {e}\")\n",
    "                prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "        else:\n",
    "            prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "        for params in tqdm(param_grid, desc=f\"Grid undersampling {feature_type} {labeltag}\"):\n",
    "            params_key = str(params)\n",
    "            if params_key in already_done:\n",
    "                continue\n",
    "            fold_accs, fold_f1_macros, fold_f1s_per_class = [], [], []\n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_tab, labels)):\n",
    "                train_texts = [texts[idx] for idx in train_idx]\n",
    "                val_texts = [texts[idx] for idx in val_idx]\n",
    "                train_tokenized = tokenizer(\n",
    "                    train_texts,\n",
    "                    add_special_tokens=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LEN,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='np'\n",
    "                )\n",
    "                val_tokenized = tokenizer(\n",
    "                    val_texts,\n",
    "                    add_special_tokens=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LEN,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                train_input_ids = train_tokenized['input_ids']\n",
    "                train_attention_mask = train_tokenized['attention_mask']\n",
    "                val_input_ids = val_tokenized['input_ids']\n",
    "                val_attention_mask = val_tokenized['attention_mask']\n",
    "                train_tabular, val_tabular = X_tab[train_idx], X_tab[val_idx]\n",
    "                train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
    "\n",
    "                tabular_sampl, input_ids_sampl, att_mask_sampl, labels_sampl = undersample_multi(\n",
    "                    train_tabular, train_labels,\n",
    "                    train_input_ids, train_attention_mask,\n",
    "                    random_state=SEED\n",
    "                )\n",
    "                train_input_ids_sampl = torch.tensor(\n",
    "                    input_ids_sampl, dtype=torch.long)\n",
    "                train_attention_mask_sampl = torch.tensor(\n",
    "                    att_mask_sampl, dtype=torch.long)\n",
    "                tabular_sampl = tabular_sampl.astype(np.float32)\n",
    "\n",
    "                train_dataset = TextTabularDataset(\n",
    "                    train_input_ids_sampl, train_attention_mask_sampl, tabular_sampl, labels_sampl\n",
    "                )\n",
    "                val_dataset = TextTabularDataset(\n",
    "                    val_input_ids, val_attention_mask, val_tabular, val_labels\n",
    "                )\n",
    "                train_loader = DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "                val_loader = DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=False,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "                present_classes = np.unique(labels_sampl)\n",
    "                weights_present = compute_class_weight(\n",
    "                    class_weight='balanced',\n",
    "                    classes=present_classes,\n",
    "                    y=labels_sampl\n",
    "                )\n",
    "                full_weights = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "                for cls, w in zip(present_classes, weights_present):\n",
    "                    full_weights[int(cls)] = w\n",
    "                fold_class_weights = torch.tensor(\n",
    "                    full_weights, dtype=torch.float32, device=DEVICE)\n",
    "                model = RobertaWithTabularDeepHead(\n",
    "                    tabular_dim=train_tabular.shape[1],\n",
    "                    num_classes=NUM_CLASSES,\n",
    "                    dropout=params['dropout']\n",
    "                )\n",
    "                val_acc, val_f1_macro, val_f1_per_class = train_and_validate(\n",
    "                    model,\n",
    "                    train_loader,\n",
    "                    val_loader,\n",
    "                    class_weights=fold_class_weights,\n",
    "                    epochs=params['epochs'],\n",
    "                    lr=params['lr']\n",
    "                )\n",
    "                fold_accs.append(val_acc)\n",
    "                fold_f1_macros.append(val_f1_macro)\n",
    "                fold_f1s_per_class.append(val_f1_per_class)\n",
    "            if len(fold_accs) == 0:\n",
    "                print(\n",
    "                    f\"[WARN] Ningún fold válido para {params} en {labeltag} con {feature_type}\")\n",
    "                continue\n",
    "            mean_acc = float(np.mean(fold_accs))\n",
    "            mean_f1_macro = float(np.mean(fold_f1_macros))\n",
    "            std_acc = float(np.std(fold_accs))\n",
    "            mean_f1_per_class = np.mean(fold_f1s_per_class, axis=0)\n",
    "            mean_f1_per_class_dict = {f'clase_{cl}': float(f1) for cl, f1 in zip(\n",
    "                sorted(np.unique(labels)), mean_f1_per_class)}\n",
    "            result = {\n",
    "                'params': params,\n",
    "                'mean_acc': mean_acc,\n",
    "                'mean_f1_macro': mean_f1_macro,\n",
    "                'std_acc': std_acc,\n",
    "                'mean_f1_per_class': mean_f1_per_class_dict,\n",
    "                'fold_results_acc': [float(x) for x in fold_accs],\n",
    "                'fold_results_f1_macro': [float(x) for x in fold_f1_macros],\n",
    "                'fold_results_f1_per_class': [f1.tolist() for f1 in fold_f1s_per_class]\n",
    "            }\n",
    "            prev_res[\"results\"][params_key] = result\n",
    "            with open(save_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(prev_res, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"[OK] Resultados guardados en {save_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df7fa9a",
   "metadata": {},
   "source": [
    "### Orientación del sesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3cd501",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"PlanTL-GOB-ES/roberta-base-bne\"\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "news_loc = './csv/news.csv'\n",
    "phrases_loc = './csv/processed_phrases_embed.csv'\n",
    "\n",
    "pairs = obtener_parejas_berta(news_loc, phrases_loc)\n",
    "\n",
    "grid_search_params = {\n",
    "    'lr':        [2e-5, 5e-5],\n",
    "    'dropout':   [0.1, 0.3],\n",
    "    'epochs':    [5],\n",
    "}\n",
    "param_grid = list(ParameterGrid(grid_search_params))\n",
    "\n",
    "BATCH_SIZE_CONFIG = {\n",
    "    \"news\": 32,\n",
    "    \"phrases\": 256\n",
    "}\n",
    "MAXLEN_CONFIG = {\n",
    "    \"news\": 512,\n",
    "    \"phrases\": 128\n",
    "}\n",
    "\n",
    "for i, (df, dataset_name) in enumerate(pairs):\n",
    "    print(f\"\\n=========== Processing: {dataset_name} ===========\")\n",
    "    MAX_LEN = MAXLEN_CONFIG[dataset_name]\n",
    "    BATCH_SIZE = BATCH_SIZE_CONFIG[dataset_name]\n",
    "    df = df.fillna(0).reset_index(drop=True)\n",
    "\n",
    "    def label_cat(x):\n",
    "        if x > 0.5:\n",
    "            return 1\n",
    "        elif x < -0.5:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "    df['label_cat'] = df['label'].apply(label_cat)\n",
    "    texts = df['text'].astype(str).tolist()\n",
    "    tabular_cols = [col for col in df.columns if col not in [\n",
    "        'label', 'text', 'label_cat']]\n",
    "    tabular_data = df[tabular_cols].values\n",
    "    labels = df['label_cat'].values.astype(int)\n",
    "    scaler = StandardScaler()\n",
    "    tabular_scaled = scaler.fit_transform(tabular_data)\n",
    "    dummy_tabular = np.zeros((len(df), 1), dtype=np.float32)\n",
    "\n",
    "    for use_tabular in [True, False]:\n",
    "        feature_type = \"all\" if use_tabular else \"embedding\"\n",
    "        print(\n",
    "            f\"\\n>> Entrenando con: {'todos los atributos' if use_tabular else 'solo embeddings de RoBERTa'}\")\n",
    "        X_tab = tabular_scaled if use_tabular else dummy_tabular\n",
    "        kfold = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "        save_name = f\"resultados_{dataset_name}_undersampling_{feature_type}.json\"\n",
    "        if os.path.exists(save_name):\n",
    "            try:\n",
    "                with open(save_name, \"r\", encoding=\"utf-8\") as f:\n",
    "                    prev_res = json.load(f)\n",
    "                already_done = set(prev_res[\"results\"].keys())\n",
    "                print(\n",
    "                    f\"[INFO] {save_name} encontrado, {len(already_done)}/{len(param_grid)} combinaciones ya guardadas.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Error leyendo {save_name}: {e}\")\n",
    "                already_done = set()\n",
    "                prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "        else:\n",
    "            already_done = set()\n",
    "            prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "        for params in tqdm(param_grid, desc=f\"Grid undersampling {feature_type}\"):\n",
    "            params_key = str(params)\n",
    "            if params_key in already_done:\n",
    "                continue\n",
    "            fold_accs, fold_f1_macros, fold_f1s_per_class = [], [], []\n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_tab, labels)):\n",
    "                train_texts = [texts[idx] for idx in train_idx]\n",
    "                val_texts = [texts[idx] for idx in val_idx]\n",
    "                train_tokenized = tokenizer(\n",
    "                    train_texts,\n",
    "                    add_special_tokens=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LEN,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='np'\n",
    "                )\n",
    "                val_tokenized = tokenizer(\n",
    "                    val_texts,\n",
    "                    add_special_tokens=True,\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LEN,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                train_input_ids = train_tokenized['input_ids']\n",
    "                train_attention_mask = train_tokenized['attention_mask']\n",
    "                val_input_ids = val_tokenized['input_ids']\n",
    "                val_attention_mask = val_tokenized['attention_mask']\n",
    "                train_tabular, val_tabular = X_tab[train_idx], X_tab[val_idx]\n",
    "                train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
    "\n",
    "                tabular_sampl, input_ids_sampl, att_mask_sampl, labels_sampl = undersample_multi(\n",
    "                    train_tabular, train_labels, train_input_ids, train_attention_mask,\n",
    "                    random_state=SEED\n",
    "                )\n",
    "                train_input_ids_sampl = torch.tensor(\n",
    "                    input_ids_sampl, dtype=torch.long)\n",
    "                train_attention_mask_sampl = torch.tensor(\n",
    "                    att_mask_sampl, dtype=torch.long)\n",
    "                tabular_sampl = tabular_sampl.astype(np.float32)\n",
    "\n",
    "                train_dataset = TextTabularDataset(\n",
    "                    train_input_ids_sampl, train_attention_mask_sampl, tabular_sampl, labels_sampl\n",
    "                )\n",
    "                val_dataset = TextTabularDataset(\n",
    "                    val_input_ids, val_attention_mask, val_tabular, val_labels\n",
    "                )\n",
    "                train_loader = DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "                val_loader = DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=False,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "                present_classes = np.unique(labels_sampl)\n",
    "                weights_present = compute_class_weight(\n",
    "                    class_weight='balanced',\n",
    "                    classes=present_classes,\n",
    "                    y=labels_sampl\n",
    "                )\n",
    "                full_weights = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "                for cls, w in zip(present_classes, weights_present):\n",
    "                    full_weights[int(cls)] = w\n",
    "                fold_class_weights = torch.tensor(\n",
    "                    full_weights, dtype=torch.float32, device=DEVICE)\n",
    "                model = RobertaWithTabularDeepHead(\n",
    "                    tabular_dim=train_tabular.shape[1],\n",
    "                    num_classes=NUM_CLASSES,\n",
    "                    dropout=params['dropout']\n",
    "                )\n",
    "                val_acc, val_f1_macro, val_f1_per_class = train_and_validate(\n",
    "                    model,\n",
    "                    train_loader,\n",
    "                    val_loader,\n",
    "                    class_weights=fold_class_weights,\n",
    "                    epochs=params['epochs'],\n",
    "                    lr=params['lr']\n",
    "                )\n",
    "                fold_accs.append(val_acc)\n",
    "                fold_f1_macros.append(val_f1_macro)\n",
    "                fold_f1s_per_class.append(val_f1_per_class)\n",
    "            mean_acc = float(np.mean(fold_accs))\n",
    "            mean_f1_macro = float(np.mean(fold_f1_macros))\n",
    "            std_acc = float(np.std(fold_accs))\n",
    "            mean_f1_per_class = np.mean(fold_f1s_per_class, axis=0)\n",
    "            mean_f1_per_class_dict = {f'clase_{cl}': float(f1) for cl, f1 in zip(\n",
    "                sorted(np.unique(labels)), mean_f1_per_class)}\n",
    "            result = {\n",
    "                'params': params,\n",
    "                'mean_acc': mean_acc,\n",
    "                'mean_f1_macro': mean_f1_macro,\n",
    "                'std_acc': std_acc,\n",
    "                'mean_f1_per_class': mean_f1_per_class_dict,\n",
    "                'fold_results_acc': [float(x) for x in fold_accs],\n",
    "                'fold_results_f1_macro': [float(x) for x in fold_f1_macros],\n",
    "                'fold_results_f1_per_class': [f1.tolist() for f1 in fold_f1s_per_class]\n",
    "            }\n",
    "            prev_res[\"results\"][params_key] = result\n",
    "            with open(save_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(prev_res, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"[OK] Resultados guardados en {save_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11008b22",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_tabular(X, y, random_state=42):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    min_count = np.min(counts)\n",
    "    sampling_strategy = {cls: min_count for cls in classes}\n",
    "    rus = RandomUnderSampler(\n",
    "        sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "    X_res, y_res = rus.fit_resample(X, y)\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c48c0a",
   "metadata": {},
   "source": [
    "### tipo 5 de sesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5223dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "\n",
    "def procesar_tipos(ruta_phrases: str):\n",
    "    df_ptype = pd.read_csv(ruta_phrases)\n",
    "    df_ptype['bias_type'] = df_ptype['bias_type'].astype(str)\n",
    "    labeltags = []\n",
    "    for n in range(1, 6):\n",
    "        col_name = f'label_{n}'\n",
    "        labeltags.append(col_name)\n",
    "        df_ptype[col_name] = df_ptype['bias_type'].apply(\n",
    "            lambda x: str(n) in x.split(',')).astype(int)\n",
    "    df_ptype = df_ptype.drop(columns=['idNew', 'idPhrase', 'bias_type'])\n",
    "    df_ptype = df_ptype.fillna(0)\n",
    "    return df_ptype, labeltags\n",
    "\n",
    "\n",
    "phrases_loc = './csv/processed_phrases_embed.csv'\n",
    "df_ptype, labeltags = procesar_tipos(phrases_loc)\n",
    "first_emb = df_ptype.columns.get_loc('emb_0')\n",
    "last_emb = df_ptype.columns.get_loc('emb_383')\n",
    "df_noemb = df_ptype.drop(df_ptype.columns[first_emb:last_emb+1], axis=1)\n",
    "X = df_noemb.drop(columns=labeltags)\n",
    "tabular_cols = [col for col in X.columns if col != 'text']\n",
    "text_col = 'text'\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[tabular_cols] = scaler.fit_transform(X_scaled[tabular_cols])\n",
    "X_scaled[text_col] = X[text_col]\n",
    "\n",
    "grid_search_params = {\n",
    "    'learning_rate':   [0.05, 0.1],\n",
    "    'num_leaves':      [15, 31],\n",
    "    'max_depth':       [-1, 5],\n",
    "    'n_estimators':    [100],\n",
    "    'min_child_samples': [10],\n",
    "    'subsample':       [1.0],\n",
    "    'colsample_bytree': [1.0],\n",
    "    'reg_alpha':       [0],\n",
    "    'reg_lambda':      [0],\n",
    "}\n",
    "param_grid = list(ParameterGrid(grid_search_params))\n",
    "\n",
    "labeltags = [\"label_5\"]\n",
    "for labeltag in labeltags:\n",
    "    print(f\"\\n=== Grid search LightGBM tabular para {labeltag} ===\")\n",
    "    y = df_ptype[labeltag].values.astype(int)\n",
    "    df_actual = X_scaled.copy()\n",
    "    df_actual['label'] = y\n",
    "    tabular_data = df_actual[tabular_cols].values\n",
    "    labels = y\n",
    "\n",
    "    feature_type = \"tabular\"\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    save_name = f\"comparativa_{labeltag}_{feature_type}_undersampling_lightgbm.json\"\n",
    "    already_done = set()\n",
    "    if os.path.exists(save_name):\n",
    "        try:\n",
    "            with open(save_name, \"r\", encoding=\"utf-8\") as f:\n",
    "                prev_res = json.load(f)\n",
    "            already_done = set(prev_res[\"results\"].keys())\n",
    "            print(\n",
    "                f\"[INFO] {save_name} encontrado, {len(already_done)}/{len(param_grid)} combinaciones ya guardadas.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Error leyendo {save_name}: {e}\")\n",
    "            prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "    else:\n",
    "        prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "\n",
    "    for params in tqdm(param_grid, desc=f\"Grid undersampling LightGBM {feature_type} {labeltag}\"):\n",
    "        params_key = str(params)\n",
    "        if params_key in already_done:\n",
    "            continue\n",
    "        fold_accs, fold_f1_macros, fold_f1s_per_class = [], [], []\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(tabular_data, labels)):\n",
    "            X_train, y_train = tabular_data[train_idx], labels[train_idx]\n",
    "            X_val, y_val = tabular_data[val_idx], labels[val_idx]\n",
    "            X_train_bal, y_train_bal = undersample_tabular(\n",
    "                X_train, y_train, random_state=SEED)\n",
    "            clf = lgb.LGBMClassifier(\n",
    "                device=\"gpu\",\n",
    "                objective='binary',\n",
    "                random_state=SEED,\n",
    "                verbose=-1,\n",
    "                **params\n",
    "            )\n",
    "            clf.fit(X_train_bal, y_train_bal)\n",
    "            val_preds = clf.predict(X_val)\n",
    "            val_acc = accuracy_score(y_val, val_preds)\n",
    "            val_f1_macro = f1_score(y_val, val_preds, average='macro')\n",
    "            val_f1_per_class = f1_score(y_val, val_preds, average=None)\n",
    "            fold_accs.append(val_acc)\n",
    "            fold_f1_macros.append(val_f1_macro)\n",
    "            fold_f1s_per_class.append(val_f1_per_class)\n",
    "        if len(fold_accs) == 0:\n",
    "            print(\n",
    "                f\"[WARN] Ningún fold válido para {params} en {labeltag} con {feature_type}\")\n",
    "            continue\n",
    "        mean_acc = float(np.mean(fold_accs))\n",
    "        mean_f1_macro = float(np.mean(fold_f1_macros))\n",
    "        std_acc = float(np.std(fold_accs))\n",
    "        mean_f1_per_class = np.mean(fold_f1s_per_class, axis=0)\n",
    "        mean_f1_per_class_dict = {f'clase_{cl}': float(f1) for cl, f1 in zip(\n",
    "            sorted(np.unique(labels)), mean_f1_per_class)}\n",
    "        result = {\n",
    "            'params': params,\n",
    "            'mean_acc': mean_acc,\n",
    "            'mean_f1_macro': mean_f1_macro,\n",
    "            'std_acc': std_acc,\n",
    "            'mean_f1_per_class': mean_f1_per_class_dict,\n",
    "            'fold_results_acc': [float(x) for x in fold_accs],\n",
    "            'fold_results_f1_macro': [float(x) for x in fold_f1_macros],\n",
    "            'fold_results_f1_per_class': [f1.tolist() for f1 in fold_f1s_per_class]\n",
    "        }\n",
    "        prev_res[\"results\"][params_key] = result\n",
    "        with open(save_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(prev_res, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"[OK] Resultados guardados en {save_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156cf02f",
   "metadata": {},
   "source": [
    "### orientación de sesgo (documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c062e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "news_loc = './csv/news.csv'\n",
    "phrases_loc = './csv/processed_phrases_embed.csv'\n",
    "\n",
    "\n",
    "def obtener_parejas_tabular(news_path, phrases_path):\n",
    "    df_news = pd.read_csv(news_path).fillna(0)\n",
    "    df_phrases = pd.read_csv(phrases_path).fillna(0)\n",
    "    drop_news = ['idNew', 'fecha', 'newspaper', 'count_X',\n",
    "                 'count_X_log', 'ratio_words_X', 'ratio_sentences_X']\n",
    "    first_emb_news = df_news.columns.get_loc('emb_0')\n",
    "    last_emb_news = df_news.columns.get_loc('emb_383')\n",
    "    emb_cols_news = df_news.columns[first_emb_news:last_emb_news+1]\n",
    "    df_news = df_news.drop(columns=drop_news +\n",
    "                           list(emb_cols_news), errors='ignore')\n",
    "    drop_phrases = ['idNew', 'idPhrase', 'bias_type']\n",
    "    first_emb_phrases = df_phrases.columns.get_loc('emb_0')\n",
    "    last_emb_phrases = df_phrases.columns.get_loc('emb_383')\n",
    "    emb_cols_phrases = df_phrases.columns[first_emb_phrases:last_emb_phrases+1]\n",
    "    df_phrases = df_phrases.drop(\n",
    "        columns=drop_phrases + list(emb_cols_phrases), errors='ignore')\n",
    "\n",
    "    def label_cat(x):\n",
    "        if x > 0.5:\n",
    "            return 1\n",
    "        elif x < -0.5:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "    df_phrases['label_cat'] = df_phrases['bias'].apply(label_cat)\n",
    "    df_news['label_cat'] = df_news['label'].apply(label_cat)\n",
    "    pairs = [(df_news, \"news\")]\n",
    "    return pairs\n",
    "\n",
    "\n",
    "grid_search_params = {\n",
    "    'learning_rate':   [0.05, 0.1],\n",
    "    'num_leaves':      [15, 31],\n",
    "    'max_depth':       [-1, 5],\n",
    "    'n_estimators':    [100],\n",
    "    'min_child_samples': [10],\n",
    "    'subsample':       [1.0],\n",
    "    'colsample_bytree': [1.0],\n",
    "    'reg_alpha':       [0],\n",
    "    'reg_lambda':      [0],\n",
    "}\n",
    "param_grid = list(ParameterGrid(grid_search_params))\n",
    "\n",
    "pairs = obtener_parejas_tabular(news_loc, phrases_loc)\n",
    "\n",
    "for df, dataset_name in pairs:\n",
    "    print(f\"\\n=== Grid search LightGBM tabular para {dataset_name} ===\")\n",
    "    y = df['label_cat'].values.astype(int)\n",
    "    tabular_cols = [col for col in df.columns if col not in [\n",
    "        'label', 'label_cat', 'text']]\n",
    "    X = df[tabular_cols].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    kfold = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    save_name = f\"comparativa_{dataset_name}_tabular_undersampling_lightgbm.json\"\n",
    "    already_done = set()\n",
    "    if os.path.exists(save_name):\n",
    "        try:\n",
    "            with open(save_name, \"r\", encoding=\"utf-8\") as f:\n",
    "                prev_res = json.load(f)\n",
    "            already_done = set(prev_res[\"results\"].keys())\n",
    "            print(\n",
    "                f\"[INFO] {save_name} encontrado, {len(already_done)}/{len(param_grid)} combinaciones ya guardadas.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Error leyendo {save_name}: {e}\")\n",
    "            prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "    else:\n",
    "        prev_res = {\"params_grid\": param_grid, \"results\": {}}\n",
    "\n",
    "    for params in tqdm(param_grid, desc=f\"Grid undersampling LightGBM tabular {dataset_name}\"):\n",
    "        params_key = str(params)\n",
    "        if params_key in already_done:\n",
    "            continue\n",
    "        fold_accs, fold_f1_macros, fold_f1s_per_class = [], [], []\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_scaled, y)):\n",
    "            X_train, y_train = X_scaled[train_idx], y[train_idx]\n",
    "            X_val, y_val = X_scaled[val_idx], y[val_idx]\n",
    "            X_train_bal, y_train_bal = undersample_tabular(\n",
    "                X_train, y_train, random_state=SEED)\n",
    "            clf = lgb.LGBMClassifier(\n",
    "                device=\"gpu\",\n",
    "                objective='multiclass',\n",
    "                num_class=3,\n",
    "                random_state=SEED,\n",
    "                verbose=-1,\n",
    "                **params\n",
    "            )\n",
    "            clf.fit(X_train_bal, y_train_bal)\n",
    "            val_preds = clf.predict(X_val)\n",
    "            val_acc = accuracy_score(y_val, val_preds)\n",
    "            val_f1_macro = f1_score(y_val, val_preds, average='macro')\n",
    "            val_f1_per_class = f1_score(y_val, val_preds, average=None)\n",
    "            fold_accs.append(val_acc)\n",
    "            fold_f1_macros.append(val_f1_macro)\n",
    "            fold_f1s_per_class.append(val_f1_per_class)\n",
    "        mean_acc = float(np.mean(fold_accs))\n",
    "        mean_f1_macro = float(np.mean(fold_f1_macros))\n",
    "        std_acc = float(np.std(fold_accs))\n",
    "        mean_f1_per_class = np.mean(fold_f1s_per_class, axis=0)\n",
    "        mean_f1_per_class_dict = {f'clase_{cl}': float(\n",
    "            f1) for cl, f1 in enumerate(mean_f1_per_class)}\n",
    "        result = {\n",
    "            'params': params,\n",
    "            'mean_acc': mean_acc,\n",
    "            'mean_f1_macro': mean_f1_macro,\n",
    "            'std_acc': std_acc,\n",
    "            'mean_f1_per_class': mean_f1_per_class_dict,\n",
    "            'fold_results_acc': [float(x) for x in fold_accs],\n",
    "            'fold_results_f1_macro': [float(x) for x in fold_f1_macros],\n",
    "            'fold_results_f1_per_class': [f1.tolist() for f1 in fold_f1s_per_class]\n",
    "        }\n",
    "        prev_res[\"results\"][params_key] = result\n",
    "        with open(save_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(prev_res, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"[OK] Resultados guardados en {save_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
